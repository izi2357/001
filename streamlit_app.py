import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import altair as alt
import time
import zipfile
import base64

# Page title
st.set_page_config(page_title='IZI MACHINE LEARNING', page_icon='ðŸ¤–')
st.set_page_config(page_title='IZI MACHINE LEARNING', page_icon='ðŸ¤–', layout='wide')
st.title('ðŸ¤– IZI MACHINE LEARNING')

with st.expander('About this app'):
Â st.markdown('**What can this app do?**')
Â st.info('This app allow users to build a machine learning (ML) model in an end-to-end workflow. Particularly, this encompasses data upload, data pre-processing, ML model building and post-model analysis.')

Â st.markdown('**How to use the app?**')
Â st.warning('To engage with the app, go to the sidebar and 1. Select a data set and 2. Adjust the model parameters by adjusting the various slider widgets. As a result, this would initiate the ML model building process, display the model results as well as allowing users to download the generated models and accompanying data.')

Â st.markdown('**Under the hood**')
Â st.markdown('Data sets:')
Â st.code('''- Drug solubility data set
Â ''', language='markdown')
Â Â 
Â st.markdown('Libraries used:')
Â st.code('''- Pandas for data wrangling
Â Â st.markdown('**What can this app do?**')
Â Â st.info('This app allows users to build a machine learning (ML) model in an end-to-end workflow. This includes data upload, data pre-processing, ML model building, and post-model analysis.')

Â Â st.markdown('**How to use the app?**')
Â Â st.warning('To engage with the app, go to the sidebar and: 1. Select a data set, 2. Adjust the model parameters using the various slider widgets. This will initiate the ML model building process, display the model results, and allow users to download the generated models and accompanying data.')

Â Â st.markdown('**Under the hood**')
Â Â st.markdown('Data sets:')
Â Â st.code('''- Drug solubility data set
Â Â ''', language='markdown')

Â Â st.markdown('Libraries used:')
Â Â st.code('''- Pandas for data wrangling
- Scikit-learn for building a machine learning model
- Altair for chart creation
- Streamlit for user interface
Â ''', language='markdown')
Â Â ''', language='markdown')


# Sidebar for accepting input parameters
with st.sidebar:
Â Â # Load data
Â Â st.header('1.1. Input data')
Â Â st.markdown('**1. Use custom data**')
Â Â uploaded_file = st.file_uploader("Upload a CSV file", type=["csv"])
Â Â if uploaded_file is not None:
Â Â Â Â df = pd.read_csv(uploaded_file, index_col=False)
Â Â Â Â 

Â Â # Download example data
Â Â st.cache_data
Â Â def convert_df(input_df):
Â Â Â Â return input_df.to_csv(index=False).encode('utf-8')

Â Â example_csv = pd.read_csv('https://raw.githubusercontent.com/dataprofessor/data/master/delaney_solubility_with_descriptors.csv')
Â Â csv = convert_df(example_csv)
Â Â st.download_button(
Â Â Â Â label="Download example CSV",
Â Â Â Â data=csv,
Â Â Â Â file_name='delaney_solubility_with_descriptors.csv',
Â Â Â Â mime='text/csv',
Â Â )
Â Â # Select example data
Â Â st.markdown('**1.2. Use example data**')
Â Â example_data = st.toggle('Load example data')
Â Â if example_data:
Â Â Â Â df = pd.read_csv('https://raw.githubusercontent.com/dataprofessor/data/master/delaney_solubility_with_descriptors.csv')
Â Â st.header('2. Set Parameters')
Â Â parameter_split_size = st.slider('Data split ratio (% for Training Set)', 10, 90, 80, 5)
Â Â st.subheader('2.1. Learning Parameters')
Â Â with st.expander('See parameters'):
Â Â Â Â parameter_n_estimators = st.slider('Number of estimators (n_estimators)', 0, 1000, 100, 100)
Â Â Â Â parameter_max_features = st.select_slider('Max features (max_features)', options=['all', 'sqrt', 'log2'])
Â Â Â Â parameter_min_samples_split = st.slider('Minimum number of samples required to split an internal node (min_samples_split)', 2, 10, 2, 1)
Â Â Â Â parameter_min_samples_leaf = st.slider('Minimum number of samples required to be at a leaf node (min_samples_leaf)', 1, 10, 2, 1)
Â Â st.subheader('2.2. General Parameters')
Â Â with st.expander('See parameters', expanded=False):
Â Â Â Â parameter_random_state = st.slider('Seed number (random_state)', 0, 1000, 42, 1)
Â Â Â Â parameter_criterion = st.select_slider('Performance measure (criterion)', options=['squared_error', 'absolute_error', 'friedman_mse'])
Â Â Â Â parameter_bootstrap = st.select_slider('Bootstrap samples when building trees (bootstrap)', options=[True, False])
Â Â Â Â parameter_oob_score = st.select_slider('Whether to use out-of-bag samples to estimate the R^2 on unseen data (oob_score)', options=[False, True])

Â Â sleep_time = st.slider('Sleep time', 0, 3, 0)

# Model selection
st.sidebar.header('3. Select Model')
model_type = st.sidebar.selectbox("Choose a model type", ("Random Forest", "Linear Regression"))

# Initiate the model building process
if uploaded_file or example_data:Â 
Â Â with st.status("Running ...", expanded=True) as status:
Â Â with st.spinner("Running ..."):

Â Â Â Â st.write("Loading data ...")
Â Â Â Â time.sleep(sleep_time)
Â Â Â Â st.write("Preparing data ...")
Â Â Â Â time.sleep(sleep_time)
Â Â Â Â X = df.iloc[:,:-1]
Â Â Â Â y = df.iloc[:,-1]
Â Â Â Â Â Â Â 
Â Â Â Â st.write("Splitting data ...")
Â Â Â Â time.sleep(sleep_time)
Â Â Â Â X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(100-parameter_split_size)/100, random_state=parameter_random_state)
Â Â Â 
Â Â Â Â st.write("Model training ...")
Â Â Â Â time.sleep(sleep_time)

Â Â Â Â if parameter_max_features == 'all':
Â Â Â Â Â Â parameter_max_features = None
Â Â Â Â Â Â parameter_max_features_metric = X.shape[1]

Â Â Â Â rf = RandomForestRegressor(
Â Â Â Â Â Â Â Â n_estimators=parameter_n_estimators,
Â Â Â Â Â Â Â Â max_features=parameter_max_features,
Â Â Â Â Â Â Â Â min_samples_split=parameter_min_samples_split,
Â Â Â Â Â Â Â Â min_samples_leaf=parameter_min_samples_leaf,
Â Â Â Â Â Â Â Â random_state=parameter_random_state,
Â Â Â Â Â Â Â Â criterion=parameter_criterion,
Â Â Â Â Â Â Â Â bootstrap=parameter_bootstrap,
Â Â Â Â Â Â Â Â oob_score=parameter_oob_score)
Â Â Â Â rf.fit(X_train, y_train)
Â Â Â Â if model_type == "Random Forest":
Â Â Â Â Â Â if parameter_max_features == 'all':
Â Â Â Â Â Â Â Â parameter_max_features = None
Â Â Â Â Â Â Â Â parameter_max_features_metric = X.shape[1]

Â Â Â Â Â Â model = RandomForestRegressor(
Â Â Â Â Â Â Â Â Â Â n_estimators=parameter_n_estimators,
Â Â Â Â Â Â Â Â Â Â max_features=parameter_max_features,
Â Â Â Â Â Â Â Â Â Â min_samples_split=parameter_min_samples_split,
Â Â Â Â Â Â Â Â Â Â min_samples_leaf=parameter_min_samples_leaf,
Â Â Â Â Â Â Â Â Â Â random_state=parameter_random_state,
Â Â Â Â Â Â Â Â Â Â criterion=parameter_criterion,
Â Â Â Â Â Â Â Â Â Â bootstrap=parameter_bootstrap,
Â Â Â Â Â Â Â Â Â Â oob_score=parameter_oob_score)
Â Â Â Â elif model_type == "Linear Regression":
Â Â Â Â Â Â model = LinearRegression()

Â Â Â Â model.fit(X_train, y_train)

Â Â Â Â st.write("Applying model to make predictions ...")
Â Â Â Â time.sleep(sleep_time)
Â Â Â Â y_train_pred = rf.predict(X_train)
Â Â Â Â y_test_pred = rf.predict(X_test)
Â Â Â Â y_train_pred = model.predict(X_train)
Â Â Â Â y_test_pred = model.predict(X_test)

Â Â Â Â st.write("Evaluating performance metrics ...")
Â Â Â Â time.sleep(sleep_time)
Â Â Â Â train_mse = mean_squared_error(y_train, y_train_pred)
Â Â Â Â train_r2 = r2_score(y_train, y_train_pred)
Â Â Â Â test_mse = mean_squared_error(y_test, y_test_pred)
Â Â Â Â test_r2 = r2_score(y_test, y_test_pred)
Â Â Â Â Â 
Â Â Â Â st.write("Displaying performance metrics ...")
Â Â Â Â time.sleep(sleep_time)
Â Â Â Â parameter_criterion_string = ' '.join([x.capitalize() for x in parameter_criterion.split('_')])
Â Â Â Â #if 'Mse' in parameter_criterion_string:
Â Â Â Â #Â Â parameter_criterion_string = parameter_criterion_string.replace('Mse', 'MSE')
Â Â Â Â rf_results = pd.DataFrame(['Random forest', train_mse, train_r2, test_mse, test_r2]).transpose()
Â Â Â Â rf_results.columns = ['Method', f'Training {parameter_criterion_string}', 'Training R2', f'Test {parameter_criterion_string}', 'Test R2']
Â Â Â Â # Convert objects to numerics
Â Â Â Â for col in rf_results.columns:
Â Â Â Â Â Â rf_results[col] = pd.to_numeric(rf_results[col], errors='ignore')
Â Â Â Â # Round to 3 digits
Â Â Â Â rf_results = rf_results.round(3)
Â Â Â Â results = pd.DataFrame([['Random forest' if model_type == "Random Forest" else 'Linear Regression', train_mse, train_r2, test_mse, test_r2]]).transpose()
Â Â Â Â results.columns = ['Method', f'Training {parameter_criterion_string}', 'Training R2', f'Test {parameter_criterion_string}', 'Test R2']
Â Â Â Â results = results.round(3)

Â Â status.update(label="Status", state="complete", expanded=False)

Â Â # Display data info
Â Â st.header('Input data', divider='rainbow')
Â Â st.header('Input data')
Â Â col = st.columns(4)
Â Â col[0].metric(label="No. of samples", value=X.shape[0], delta="")
Â Â col[1].metric(label="No. of X variables", value=X.shape[1], delta="")
Â Â col[2].metric(label="No. of Training samples", value=X_train.shape[0], delta="")
Â Â col[3].metric(label="No. of Test samples", value=X_test.shape[0], delta="")
Â Â Â 
Â Â with st.expander('Initial dataset', expanded=True):
Â Â Â Â st.dataframe(df, height=210, use_container_width=True)
Â Â with st.expander('Train split', expanded=False):
Â Â Â Â train_col = st.columns((3,1))
Â Â Â Â with train_col[0]:
Â Â Â Â Â Â st.markdown('**X**')
Â Â Â Â Â Â st.dataframe(X_train, height=210, hide_index=True, use_container_width=True)
Â Â Â Â with train_col[1]:
Â Â Â Â Â Â st.markdown('**y**')
Â Â Â Â Â Â st.dataframe(y_train, height=210, hide_index=True, use_container_width=True)
Â Â with st.expander('Test split', expanded=False):
Â Â Â Â test_col = st.columns((3,1))
Â Â Â Â with test_col[0]:
Â Â Â Â Â Â st.markdown('**X**')
Â Â Â Â Â Â st.dataframe(X_test, height=210, hide_index=True, use_container_width=True)
Â Â Â Â with test_col[1]:
Â Â Â Â Â Â st.markdown('**y**')
Â Â Â Â Â Â st.dataframe(y_test, height=210, hide_index=True, use_container_width=True)
Â Â # Zip dataset files
Â Â df.to_csv('dataset.csv', index=False)
Â Â X_train.to_csv('X_train.csv', index=False)
Â Â y_train.to_csv('y_train.csv', index=False)
Â Â X_test.to_csv('X_test.csv', index=False)
Â Â y_test.to_csv('y_test.csv', index=False)
Â Â Â 
Â Â list_files = ['dataset.csv', 'X_train.csv', 'y_train.csv', 'X_test.csv', 'y_test.csv']
Â Â with zipfile.ZipFile('dataset.zip', 'w') as zipF:
Â Â Â Â for file in list_files:
Â Â Â Â Â Â zipF.write(file, compress_type=zipfile.ZIP_DEFLATED)
Â Â with open('dataset.zip', 'rb') as datazip:
Â Â Â Â btn = st.download_button(
Â Â Â Â Â Â Â Â label='Download ZIP',
Â Â Â Â Â Â Â Â data=datazip,
Â Â Â Â Â Â Â Â file_name="dataset.zip",
Â Â Â Â Â Â Â Â mime="application/octet-stream"
Â Â Â Â Â Â Â Â )

Â Â # Display model parameters
Â Â st.header('Model parameters', divider='rainbow')
Â Â st.header('Model parameters')
Â Â parameters_col = st.columns(3)
Â Â parameters_col[0].metric(label="Data split ratio (% for Training Set)", value=parameter_split_size, delta="")
Â Â parameters_col[1].metric(label="Number of estimators (n_estimators)", value=parameter_n_estimators, delta="")
Â Â parameters_col[2].metric(label="Max features (max_features)", value=parameter_max_features_metric, delta="")
Â Â if model_type == "Random Forest":
Â Â Â Â parameters_col[1].metric(label="Number of estimators (n_estimators)", value=parameter_n_estimators, delta="")
Â Â Â Â parameters_col[2].metric(label="Max features (max_features)", value=parameter_max_features_metric, delta="")

Â Â # Display feature importance plot
Â Â importances = rf.feature_importances_
Â Â feature_names = list(X.columns)
Â Â forest_importances = pd.Series(importances, index=feature_names)
Â Â df_importance = forest_importances.reset_index().rename(columns={'index': 'feature', 0: 'value'})

Â Â bars = alt.Chart(df_importance).mark_bar(size=40).encode(
Â Â Â Â Â Â Â x='value:Q',
Â Â Â Â Â Â Â y=alt.Y('feature:N', sort='-x')
Â Â Â Â Â Â ).properties(height=250)

Â Â performance_col = st.columns((2, 0.2, 3))
Â Â with performance_col[0]:
Â Â Â Â st.header('Model performance', divider='rainbow')
Â Â Â Â st.dataframe(rf_results.T.reset_index().rename(columns={'index': 'Parameter', 0: 'Value'}))
Â Â with performance_col[2]:
Â Â Â Â st.header('Feature importance', divider='rainbow')
Â Â Â Â st.altair_chart(bars, theme='streamlit', use_container_width=True)
Â Â # Display feature importance plot if Random Forest
Â Â if model_type == "Random Forest":
Â Â Â Â importances = model.feature_importances_
Â Â Â Â feature_names = list(X.columns)
Â Â Â Â forest_importances = pd.Series(importances, index=feature_names)
Â Â Â Â df_importance = forest_importances.reset_index().rename(columns={'index': 'feature', 0: 'value'})

Â Â Â Â bars = alt.Chart(df_importance).mark_bar(size=40).encode(
Â Â Â Â Â Â Â Â x='value:Q',
Â Â Â Â Â Â Â Â y=alt.Y('feature:N', sort='-x')
Â Â Â Â Â Â ).properties(height=250)

Â Â Â Â performance_col = st.columns((2, 0.2, 3))
Â Â Â Â with performance_col[0]:
Â Â Â Â Â Â st.header('Model performance')
Â Â Â Â Â Â st.dataframe(results.T.reset_index().rename(columns={'index': 'Parameter', 0: 'Value'}))
Â Â Â Â with performance_col[2]:
Â Â Â Â Â Â st.header('Feature importance')
Â Â Â Â Â Â st.altair_chart(bars, theme='streamlit', use_container_width=True)

Â Â # Prediction results
Â Â st.header('Prediction results', divider='rainbow')
Â Â st.header('Prediction results')
Â Â s_y_train = pd.Series(y_train, name='actual').reset_index(drop=True)
Â Â s_y_train_pred = pd.Series(y_train_pred, name='predicted').reset_index(drop=True)
Â Â df_train = pd.DataFrame(data=[s_y_train, s_y_train_pred], index=None).T
Â Â df_train['class'] = 'train'
Â Â Â Â Â 
Â Â s_y_test = pd.Series(y_test, name='actual').reset_index(drop=True)
Â Â s_y_test_pred = pd.Series(y_test_pred, name='predicted').reset_index(drop=True)
Â Â df_test = pd.DataFrame(data=[s_y_test, s_y_test_pred], index=None).T
Â Â df_test['class'] = 'test'
Â Â Â 
Â Â df_prediction = pd.concat([df_train, df_test], axis=0)
Â Â Â 
Â Â prediction_col = st.columns((2, 0.2, 3))
Â Â Â 
Â Â # Display dataframe
Â Â with prediction_col[0]:
Â Â Â Â st.dataframe(df_prediction, height=320, use_container_width=True)
Â Â # Display scatter plot of actual vs predicted values
Â Â with prediction_col[2]:
Â Â Â Â scatter = alt.Chart(df_prediction).mark_circle(size=60).encode(
Â Â Â Â Â Â Â Â Â Â Â Â x='actual',
Â Â Â Â Â Â Â Â Â Â Â Â y='predicted',
Â Â Â Â Â Â Â Â Â Â Â Â color='class'
Â Â Â Â Â Â Â Â Â )
Â Â Â Â st.altair_chart(scatter, theme='streamlit', use_container_width=True)


# Ask for CSV upload if none is detected
else:
Â Â st.warning('ðŸ‘ˆ Upload a CSV file or click *"Load example data"* to get started!')
